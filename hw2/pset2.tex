\documentclass[12pt]{article}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}
\setlength{\textwidth}{6.5in}
\setlength{\parindent}{0in}
\setlength{\parskip}{\baselineskip}

\usepackage{amsmath,amsfonts,amssymb,mathtools,fancyvrb}


\begin{document}


CSCI 4446 Spring 2019 \hfill Problem Set 2\\
Derek Wright


\hrulefill

\begin{enumerate}

	\item	\textit{Write a program that displays \textbf{m} iterates of the logistic equation on the axes \textbf{xn} versus
\textbf{R} (a “bifurcation plot”), but add an argument \textbf{l} that allows the user to suppress the
plotting of the first \textbf{l} points (the transient). Turn in a bifurcation plot for the range
$$2.8 < R < 4$$. Pick $$l$$, $$m$$, and the interval between $$R$$ values such that the details of
the behavior are visible, but not such that the plot requires exorbitant amounts of CPU
time. You’ll need to use small point icons to get this to look reasonable—no diamonds
(}) or other symbols, just dots (·)!
As always in this course, the plots you turn in should be what your code produces, not
screenshots of some app from the web.}
	
	\begin{enumerate}
	    \item \textit{$T(n)=T(n-2)+Cn$ if $n>1$, and $T(n)=C$ otherwise}
	    
	    We will use the unrolling method for this problem: \\
	    Without loss of Generality, let's assume n is even. That is, $n = 2k \text{ for } k \in \mathbb{Z}$.
	    \begin{align*}
	    	 T(n) &= T(n-2) + Cn  \\
	    	 		&= T(n-4) + C(n-2) + Cn	 	\\
	    	 		&= T(n-6) +  C(n-4) + C(n-2) +Cn   \\
	    	 		\vdots    \\
	    	 		&= T(0) + 2C + \dots + C(n-2) + Cn
	    \end{align*}
	    
 	    This pattern continues until we reach an $k = n-i  \leq 1$, in which case T(k) = C.
 	    We can now condense this into a sum.
 	    \begin{center}
 	    Notice: $i$ is strictly even, folowing the pattern seen above. That is $i = 2k \text{ for } k \in \mathbb{Z}$.  \\ 
 	   We can iterate $i$ by 2 instead of 1 in the summation, or change the bounds to $n/2$ and subtract $2i$.\\
 	   \begin{gather*}
		    	\sum_{i = 0\text{, even i's only}}^{n} C(n-i) \text{       or      }\\
		    	\sum_{i=0}^{n/2} C(n-2i)
 	   \end{gather*}
  	  
 	    \end{center}
     \section* \nonumber
		From here we can perform the same step. First we factor out the constant C, and then invert $n-i$ into just $i$ in the summation, since both forms accomplish the same thing: sum up even integers until $n$. Because we are only adding half the numbers from $0$ to $n$, I use the common equation for sum of consequtive integers, and divide by $2$. As you will see, this does not change the asymptotics.
     \begin{gather*}
     \frac{C}{2}\sum_{i=0}^{n} (n-i) = \frac{C}{2}\sum_{i=0}^{n} i = C\frac{n(n+1)}{4} \Longrightarrow \boxed{\Theta(n^2)}
     \end{gather*}  
	    	    	
      \item \textit{$T(n)=3T(n-1)+1$ if $n>1$ and $T(1)=3$}

      Unrolling this function, we get something like:
      \begin{gather*}
         T(n) = 3(3(3T(n-3) +1)+1)+1, \text{ which can be written:}\\
         T(n) = 3^{3}T(n-3) + c_1, \text{ Or generally, for the $i^{th}$ step of recursion:}\\
		 T(n) = 3^i\cdot T(n-i) + c_1
      \end{gather*}
      Note: We absorb every $(...+1)$ term in the trailing constant, $c_1$.\\
	The recursion bottoms out when $i = n-1$:
	\begin{align*}
	T(n) &= 3^{n-1} \cdot T(1) + c_1\\
	&=(3^{n-1})(3)+ c_1 \\
	&= 3^n + c_1 \Longrightarrow \boxed{\Theta(3^n)}
	\end{align*}
	The recursion is $\Theta(3^n)$\\
	
	Another method we could have used was by summing up the cost at each level.\\
	The first level has cost $1$, the second $3$, the third $9$ and the $(n-1)^{st} \text{   }3^{n-1}$.\\ So, using the partial sum formula:
	\begin{gather*}
	\sum_{i=0}^{n-1} 3^i = \frac{3^{n-1+1}-1}{3-1} \Longrightarrow \Theta(3^n).
	\end{gather*}
	\pagebreak

          \item \textit{$T(n)=T(n-1)+3^n$ if $n>1$ and $T(1)=3$}

          For this problem, we have no splitting, so a recursion tree would look rather boring (linear). Instead, let's unroll a bit:
          \begin{align*}
          T(n)&= T(n-2) + 3^n + 3^n\\
          &= T(n-3) + 3(3^n)\\
          \vdots\\
          &= T(n- i) + i(3^n)\\
          &= T(1) + (n-1)(3^n) \Longrightarrow \boxed{ \Theta(n \cdot 3^n)}
          \end{align*}

          \item \textit{$T(n)=T(n^{1/4}+1)$ if $n>2$, and $T(n)=0$ otherwise}

          This problem is interesting because there is no cost at each level -- that is, $f(n) = 0$. There is just a call to go deeper into recursion. For this reason, for every $n$, $T(n) = 0$, because at some point ${n_i}^{1/4} +1 \leq 2$, where $n_i$ is the $n$ corresponding to the $i^{th}$ recursive call. $\boxed{\text{In a perfect world, this would be } \Theta(1)}$.
      	  With that said, each call to recursion can be counted as a $\Theta(1)$ operation, and so the total $T(n)$ complexity is simply the amount of times we need to recurse.\\
      	  Unrolling, we approximate $(n^{1/4} + 1)^{1/4} = n^{1/16}$:
      	  \begin{gather*}
      	  T(n) = T(n^{1/4} + 1) = T((n^{1/4}+1)^{1/4}) \approx T(n^{1/16}) = T(n^{1/64})  = \cdots
      	  \end{gather*}
      	  Asymptotically, they will act the same. To solve for the depth of our recursion, we can write:
      	  \begin{gather*}
      	  n^{1/(4x)} = 2\\ \text{    We set equal to 2 because that is where our recursion stops.}\\
      	  \text{Solving for x, which is the depth:}\\
      	  \frac{lgn}{4x} = \lg 2 = 1 \rightarrow x = \frac{lgn}{4} \Longrightarrow \boxed{\Theta(logn)}
      	  \end{gather*}
	
	\end{enumerate}
	
	\newpage
	
	\item \textit{Consider the following function:}
	

	\begin{verbatim}
    def foo(n) {
        if (n > 1) {
        print( ''hello'' )
        foo(n/3)
        foo(n/3)
    }}
	\end{verbatim}

      \textit{In terms of the input n, determine how many times is ``hello'' printed. Write down a recurrence and solve using the Master method.}
      
      Analyzing this function, I can see that each step we call two more recursions, with a parameter of $n/3$. Also, each step we print "hello," which takes constant ($\Theta(1)$) time.

      Recurrence : $T(n) = 2T(n/3) + \Theta(1)$
      
      Using the master method, $f(n) = 1 = n^0$. Now let's compute: $\log _3(2) = 0.631 < 1$, which tells us that $n^{\log _3(2) - \epsilon}$ is a good upper bound on our $f(n) = \Theta(1)$. This puts us in the first situation of the Master Method, resulting in:
      $\boxed{T(n) = \Theta(n^{0.631})}$
      So, "hello" will be printed $n^{\log_3 2}$ times.
	
	The above, alas, does not give an exact number of times still. To solve this, I used a recurrence tree (see Figure \ref{fig:foo_tree} below). When I draw the tree, it is split by twos each level of the tree:
	
	\begin{figure*}[!th]
		\centering
			\includegraphics[width=10cm, height=5cm]{CSCI3104_PS2_RecurTree}
			\caption{Figure \ref{fig:foo_tree} shows $3$ levels of the recurrence tree for the function foo.}
			\centering
			\label{fig:foo_tree}
	\end{figure*}
	
	 At each level (of which there are $\log_3 n$), $i$, "hello" is printed $2^i$ times. \\
	Taking the sum, using the popular partial sum formula:
	\begin{gather*}
		\sum_{i=0}^{\lfloor\log_3 n\rfloor} 2^i = \boxed{2^{{\lfloor\log_3 n\rfloor} + 1} -1} \\
		\end{gather*}
		This is our exact answer, hence the use of the floor functions. This does not throw out any constants, as the above.\\
	\\
		To show my two answers are really the same answer, I investigate below:\\
		\begin{gather*}
		2^{{\log_3 n} + 1} -1 \stackrel{?}{=} n^{\log_3 2}
		\end{gather*}
		
		First, let's throw out the low-impact terms. The right side is approximated to an extent, so I will guess for now that I can remove the $1$ terms, both in the exponent and outside:
		\begin{align*}
		2^{\log_3 n} &\stackrel{?}{=} n^{\log_3 2} \text{ Now lets take the log base 2 of both sides:}\\
		\log_3n * \lg 2 &= \lg (n^{\log_3 2}) = \log_3 2 * \lg n\\
		\log_3n &= \log_3 2 * \lg n\\
		&= \frac{\lg 2}{\lg 3}*\lg n\\
		&= \frac{\lg n}{\lg 3}\\
		&= \log_3 n
		\end{align*}
		
		The guess is good, and I am now confident in my answers. The first answer is a good large-n approximation, while the second boxed answer is the exact.
	
	\newpage

      	\item \textit{Professor McGonagall asks you to help her with some arrays that are \texttt{raludominular}. A raludominular array has the property that the subarray $A[1..i]$ has the property that $A[j] > A[j + 1]$ for $1 \le j < i$, and the subarray $A[i..n]$ has the property that $A[j] < A[j + 1]$ for $i \le j < n$. Using her wand, McGonagall writes the following raludominular array on the board $A = [7, 6, 4, -1, -2, -9, -5, -3, 10, 13]$, as an example.}

	\begin{enumerate}
	    \item \textit{Write a recursive algorithm that takes asymptotically sub-linear time to find the minimum element of A.}
	
\begin{Verbatim}[numbers=left,xleftmargin=5mm]
int minSearch(A, l, r) {
 int middle = (r-l)/2 + l; //add l to offset to array
 if (A[middle] > A[middle-1]]) {//means we need to move left
   return minSesarch(A, 0, middle-1)
   }
 else if (A[middle] > A[middle + 1]
   return minSearch(A, middle+1, n)
   }
 else return A[middle]
}
	    \end{Verbatim}
	    
	    This pseudocode finds the midpoint of the input subarray, and asks first if this middle value is larger than the one to its left. If this is the case, we need to move left in the array, so we recurse with the left half. If the middle value is smaller than the input to its left, we ask is it also greater than the value to its right. If this is the case, we must move right in the array, and recurse. If neither of these if statements get called, we must be at the minimum value. This is similar to the first derivative test. We are smaller than both neighbors, and are at a local minimum.
	    
	    To prove this is sublinear, let's write a recursion and use the master method:\\
	    
	    Recurrence: $T(n) = T(n/2) + \Theta(1)$
	    
	    Master Method: $n^{\lg 1} = n^0$\\
	    Since $f(n)= \Theta(n^0) = \Theta(1)$, we know that $T(n)$ is $\boxed{\Theta(n^{\lg 1}\cdot \lg n) = \Theta(\log n)) = o(n)}$
	
          \item \textit{Prove that your algorithm is correct. (Hint: prove that your algorithm's correctness follows from the correctness of another correct algorithm we already know.)}\\
          
          The pseudocode above is very reminiscent of binary search. We can use the same loop invariant: That the minimum value of the array is in the subarray $A[l ... r]$.
          \textbf{Init}: At initialization, the subarray is equal to the full array. Since every array must have a minimum value, the loop invariant is true.\\
          \textbf{Maintenance}: Let's assume the absolute minimum value of array $A$ is within the $k^{th}$ subarray $A[l...r]$. We find the middle index of the array, and compare to each of its neighbors. If the middle index is greater than its left neighbor, the minimum must be to the left of middle. This is because we know the array is ordered until we reach the minimum. So we follow the path of getting smaller. It is now guaranteed that the $k+1^{st}$ subarray, A[l...(r-l)/2 +l]contains the minimum.\\
          If, however, the middle value is smaller than its left neighbor, we check its right neighbor. Following the path of getting smaller, if the right neighbor is smaller than the middle, we recurse with the $k+1^{st}$ subarry being $A[(r-l/)/2 + l]$. It is again guaranteed to contain the minimum.\\
          \textbf{Termination}: The recursion terminates when the calculated middle is greater than neither of its neighbors. If this is the case. The middle value is the minimum, and we return. $\boxdot$
	
          \item \textit{Now consider the \texttt{multi-raludominular} generalization, in which the array contains k local minima, i.e., it contains k subarrays, each of which is itself a raludominular array. Let $k = 2$ and prove that your algorithm can fail on such an input.}

          Consider the array $A = [6, 4, -1, -2, 1, 3, 4, -9, -5, 3]$. There are $10$ numbers in this array, with two local minima, $-2$ and $-9$. The algorithm above will look at the $5^{th}$ element, which is $1$. Because $1 > -2$, We will recurse on the left half of the array only, and return a value of $-2$, which is greater than $-9$, the minimum value of the array.

          \item \textit{Suppose that $k = 2$ and we can guarantee that neither local minimum is closer than $n=3$ positions to the middle of the array, and that the ``joining point'' of the two singly-raludominular subarrays lays in the middle third of the array. Now write an algorithm that returns the minimum element of A in sublinear time. Prove that your algorithm is correct, give a recurrence relation for its running time, and solve for its asymptotic behavior.}

		Below is the pseudocode. Of note is that it is not a recursive function, and thus doesn't have a recurrence relation, but we can still analyze the runtime:

\pagebreak
          \begin{Verbatim}[numbers=left,xleftmargin=5mm]
int doubleMinSearch(A) {
    middle = Length(A)/2
    min1 = minSearch(A, 0, middle-1)
    min2 = minSearch(A, middle, Length(A)-1)
    if min1 < min2
      return min1
    else
      return min2
}
        \end{Verbatim}
	\textbf{Proving correctness}:
	Because we have proved the correctness of $minSearch$, the correctness of this algorithm immediately follows: The above code immediately splits the array into two, and calls two separate $minSearch$'s.   We know that one local minimum is in the range of $A[0 ... middle-1]$ and we know the other is in the range of $A[middle ... Length(A)-1]$. We are without fear that we might catch both minima in one array because the problem stated they were well spaced and safely away from the middle.  The two searches return their respective local minimum, and we simply ask which one of our minima is the smaller, and return that value.\\
	\textbf{Proving runtime}: As stated above, there is no recurrence relation. So instead, let's add up the cost at each step:\\
	
	Line 2 is $\Theta(1)$.
	Line 3 adds $\Theta(\log n)$.
	Line 4 adds a second $\Theta(\log n)$.
	Finally, Line 5 adds one last $\Theta(1)$.\\
	In total, our runtime is:\\
	 $T(n) = \Theta(1+\log n + \log n + 1) = \Theta(2\log n + 2) \Longrightarrow \boxed{\Theta(\log n)}$
	
	\end{enumerate}

	\newpage
       \item \textit{Asymptotic relations like $O$, $\Omega$, and $\Theta$ represent relationships between functions, and these relationships are transitive. That is, if some $f(n) = \Omega(g(n))$, and $g(n) = \Omega(h(n))$, then it is also true that $f(n) =  \Omega(h(n))$. This means that we can sort functions by their asymptotic growth.\footnotemark}

     \textit{Sort the following functions by order of asymptotic growth such that the final arrangement of functions $g_1,g_2 \dots,g_{12}$ satisfies the ordering constraint $g_1 = \Omega(g2)$, $g_2 = \Omega(g_3)$, \dots , $g_{11}=\Omega(g_{12})$}.


     \begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|}
     	\hline
     	$1$&
     	$2$&
     	$3$&
     	$4$&
     	$5$&
     	$6$&
     	$7$&
     	$8$&
    	$9$&
     	$10$&
    	$11$&
     	$12$\\
     \hline
      $n$ &
      $n^{1.5}$  &
      $8^{\lg n}$ &
      $4^{\lg* n}$  &
      $n!$  &
      $(\lg n)!$ &
      $(\frac{5}{4})^n$ &
      $n^{1/ \lg n}$ &
      $n \lg n$ &
      $\lg(n!)$ &
      $e^n$ &
      $42$ \\
     \hline
     \end{tabular}

      \textit{Give the final sorted list and identify which pair(s) functions $f(n)$, $g(n)$, if any, are in the same equivalence class, i.e., $f(n) = \Theta(g(n))$.}

        First, let us create a preliminary list based on intuitition alone.\\
        I am using rules such as: constants don't grow, higher exponents grow faster, exponentials grow even faster, factorials possibly more so.\\
        \begin{gather*}
        42, n^\frac{1}{lgn}, n , n^{1.5}, nlgn, 4^{lg*n}, 8^{lgn}, \left(\frac{5}{4}\right)^n, e^n, lg(n!), (lgn)!, n!
        \end{gather*}
The first ordering I'm not sure of is the $n^{\frac{1}{lgn}}$ term. Comparing to n, and using the log of limits rule we get:\\
\begin{gather*}
\lim\limits_{n\to\infty}\frac{n^{\frac{1}{lgn}}}{n} = \lim\limits_{n\to\infty}n^{1/lgn - 1} \\
\begin{align*}
log_n \left(\lim\limits_{n\to\infty}n^{1/lgn - 1}\right) &= \lim\limits_{n\to\infty}log_n(n^{1/lgn} - 1)\\
&= \lim\limits_{n\to\infty}\frac{1}{lgn} - 1 = -1\\
\end{align*}\\
n^{-1} \stackrel{n \to \infty}{\Longrightarrow} 0. \text{Therefore, $n$ grows faster}
\end{gather*}

Next we can talk about the factorials at the fast-growing end of the list. Here, it is intuitive. $n!$ grows faster than $(lgn)!$ because $n$ is \textbf{always} bigger than $lgn$, guaranteeing $n! = \Omega((\lg n)!)$. Similarly, $(lgn)!$ grows faster than $lg(n!)$ because the former acts like a factorial curve and the latter acts like a logarithmic curve. Factorials have a concave-up curve, while all $log$ functions have a concave down curve. No matter how fast the argument to the log grows, it will be concave down and thus more slow-growing than any concave-up function. This result is thanks to calculus. This also means that $e^n = \Omega((5/4)^n) = \Omega(8^{\lg n}) = \Omega(n^{1.5}) = \Omega(\lg (n!))$\\
These are all the concave-up functions. Any mistakes in the order now will hopefully be fixed below.

Testing $\lg (n!)$ with $n\lg n$:
\begin{align*}
\lim\limits_{n\to\infty}\frac{\lg (n!)}{n\lg n} &= \lim\limits_{n\to\infty}\frac{\log_n (n!)}{n}\\
&= \lim\limits_{n\to\infty}\frac{\log_n(\sqrt{2\pi n}\cdot (n/e)^n)}{n} \text{Approximating...}\\
&= \lim\limits_{n\to\infty} \frac{(n+1/2)\log_n n}{n} \Longrightarrow \boxed{1}
\end{align*}

They grow at the same rate! They are $\Theta$ of each other.

Note: To confirm the factorial results using limits, we'd have to utilize Stirling's Approximation, which says:\\
\begin{center}
	$n! \approx \sqrt{2\pi n}(\frac{n}{e})^n$ \\
	\text{Attempting to plug in...}
	\begin{align*}
		\lim\limits_{n\to\infty}\frac{n!}{(lgn)!} = 	\lim\limits_{n\to\infty}\frac{\sqrt{2\pi n}(\frac{n}{e})^n}{\sqrt{2\pi \lg n}(\frac{\lg n}{e})^{\lg n}} = \lim\limits_{n\to\infty}\frac{\sqrt{n}n^ne^{\lg n}}{\sqrt{\lg n}({\lg n})^{\lg n}e^n}
	\end{align*}
\end{center}
I struggled with it but could not knead it into something pretty. But the concavity argument, as well as the $n!$ and $(\lg n)!$ argument, seem extremely valid albethey not a proof.

Next, we already knew from previous homework and in-class lecture that $e^n$ grows faster than $\left(\frac{5}{4}\right)^n$, since $e > \frac{5}{4}$. But what about $8^{lgn} \text{ and } 4^{lg*n}$?

\begin{gather*}
	\lim\limits_{n\to\infty}\left(\frac{\frac{5}{4}^n}{8^{lgn}}\right) = \lim\limits_{n\to\infty}\left(\frac{n(\frac{5}{4})^{n-1}}{(lgn)(8)^{lgn-1}}\right) = \lim\limits_{n\to\infty}\left(\frac{n(n-1)(\frac{5}{4})^{n-2}}{(lgn)(lgn - 1)(8)^{lgn-2}}\right) = \dots
	\end{gather*}
	
	As is clear, the top will eventually end up as $n!$ (with some constant in front), while the bottom will act like $(lgn)!$, which we already know grows slower than $n!$, as we intuited above. Moving on:
	
	\begin{gather*}
		\lim\limits_{n\to\infty}\left(\frac{8^{lgn}}{4^{lg*n}}\right)\\
		\text{}
	\end{gather*}
The lg* function, or the iterated logarithm, is a very, very slow-growing function. So much so that an $n$ value of $2^{65535}$ gives a $lg*n$ value of $5$. Meanwhile, an $n$ value of $65535$ yields a value of $\approx 280$ trillion. This one is a no brainer. But now, with extra knowledge of the iterated logarithm, we have to move this function WAY down the list. All the functions on the list grow faster than the iterated logarithm, except of course for the constant, which doesn't grow at all. Even though $2^{65535}$ is a number larger than the amount of atoms in the universe, we must say that eventually $lg*(n)$ will exceed $42$, because the $\lim\limits_{n\to\infty}\lg*n = \infty$

The new and final list, switched to coincide with the $\Omega$ notation ($g_1(n) = \Omega(g_2(n))$).\\ i.e. $n! = \Omega((lgn)!)$

  \begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|}
	\hline
	$1$&
	$2$&
	$3$&
	$4$&
	$5$&
	$6$&
	$7$&
	$8$&
	$9$&
	$10$&
	$11$&
	$12$\\
	\hline
	$n!$ &
	$(\lg n)!$  &
	$e^n$  &
	$(\frac{5}{4})^n$ &
	$8^{\lg n}$  &
	$n^{1.5}$ &
	$\lg(n!)$ &
	$n \lg n$ &
	$n$ &
	$n^{1/ \lg n}$ &
	$4^{\lg* n}$ &
	$42$ \\
	\hline
\end{tabular}

\end{enumerate}
\footnotetext{The notion of sorting is entirely general: so long as you can define a pairwise comparison operator for a set of objects \texttt{S} that is transitive, then you can sort the things in \texttt{S}. For instance, for strings, we use a comparison based on lexical ordering to sort them. Furthermore, we can use any sorting algorithm to sort \texttt{S}, by simply changing the comparison operators $>$, $<$, etc. to have a meaning appropriate for \texttt{S}. For instance, using $\Omega$, $O$, and $\Theta$, you could apply QuickSort or MergeSort to the functions here to obtain the sorted list.}

\end{document}

